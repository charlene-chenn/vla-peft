{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1494,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.040211108318673035,
      "grad_norm": 23.151397705078125,
      "learning_rate": 8.444444444444444e-05,
      "loss": 28.3679,
      "step": 20
    },
    {
      "epoch": 0.08042221663734607,
      "grad_norm": 2.402862310409546,
      "learning_rate": 0.00017333333333333334,
      "loss": 20.3015,
      "step": 40
    },
    {
      "epoch": 0.1206333249560191,
      "grad_norm": 1.6668062210083008,
      "learning_rate": 0.00019995393663024054,
      "loss": 17.7249,
      "step": 60
    },
    {
      "epoch": 0.16084443327469214,
      "grad_norm": 1.218038558959961,
      "learning_rate": 0.00019972842227844787,
      "loss": 17.0002,
      "step": 80
    },
    {
      "epoch": 0.20105554159336517,
      "grad_norm": 1.8855749368667603,
      "learning_rate": 0.00019931541975950378,
      "loss": 16.7749,
      "step": 100
    },
    {
      "epoch": 0.2412666499120382,
      "grad_norm": 0.6629523634910583,
      "learning_rate": 0.00019871570551289805,
      "loss": 16.7658,
      "step": 120
    },
    {
      "epoch": 0.28147775823071125,
      "grad_norm": 1.1657142639160156,
      "learning_rate": 0.00019793040699379682,
      "loss": 16.4277,
      "step": 140
    },
    {
      "epoch": 0.3216888665493843,
      "grad_norm": 0.9205684661865234,
      "learning_rate": 0.00019696100055344124,
      "loss": 16.683,
      "step": 160
    },
    {
      "epoch": 0.3618999748680573,
      "grad_norm": 0.9021503925323486,
      "learning_rate": 0.00019580930866362604,
      "loss": 16.6944,
      "step": 180
    },
    {
      "epoch": 0.40211108318673033,
      "grad_norm": 0.6708090305328369,
      "learning_rate": 0.00019447749649047542,
      "loss": 16.6862,
      "step": 200
    },
    {
      "epoch": 0.44232219150540336,
      "grad_norm": 1.2588716745376587,
      "learning_rate": 0.0001929680678239585,
      "loss": 16.6784,
      "step": 220
    },
    {
      "epoch": 0.4825332998240764,
      "grad_norm": 0.5250007510185242,
      "learning_rate": 0.00019128386037079587,
      "loss": 16.7838,
      "step": 240
    },
    {
      "epoch": 0.5227444081427495,
      "grad_norm": 1.1095997095108032,
      "learning_rate": 0.0001894280404196069,
      "loss": 16.6343,
      "step": 260
    },
    {
      "epoch": 0.5629555164614225,
      "grad_norm": 0.7451302409172058,
      "learning_rate": 0.00018740409688832764,
      "loss": 16.6883,
      "step": 280
    },
    {
      "epoch": 0.6031666247800955,
      "grad_norm": 1.2837142944335938,
      "learning_rate": 0.00018521583476508905,
      "loss": 16.783,
      "step": 300
    },
    {
      "epoch": 0.6433777330987686,
      "grad_norm": 0.9047398567199707,
      "learning_rate": 0.00018286736795488802,
      "loss": 16.5984,
      "step": 320
    },
    {
      "epoch": 0.6835888414174416,
      "grad_norm": 0.5036806464195251,
      "learning_rate": 0.00018036311154549784,
      "loss": 16.4152,
      "step": 340
    },
    {
      "epoch": 0.7237999497361146,
      "grad_norm": 0.737265944480896,
      "learning_rate": 0.0001777077735071596,
      "loss": 16.4556,
      "step": 360
    },
    {
      "epoch": 0.7640110580547876,
      "grad_norm": 0.46105828881263733,
      "learning_rate": 0.00017490634584165773,
      "loss": 16.5862,
      "step": 380
    },
    {
      "epoch": 0.8042221663734607,
      "grad_norm": 0.57232266664505,
      "learning_rate": 0.0001719640951974202,
      "loss": 16.4969,
      "step": 400
    },
    {
      "epoch": 0.8444332746921337,
      "grad_norm": 0.4672178030014038,
      "learning_rate": 0.0001688865529682862,
      "loss": 16.6426,
      "step": 420
    },
    {
      "epoch": 0.8846443830108067,
      "grad_norm": 0.5884015560150146,
      "learning_rate": 0.00016567950489455616,
      "loss": 16.3299,
      "step": 440
    },
    {
      "epoch": 0.9248554913294798,
      "grad_norm": 0.5508126020431519,
      "learning_rate": 0.00016234898018587337,
      "loss": 16.7245,
      "step": 460
    },
    {
      "epoch": 0.9650665996481528,
      "grad_norm": 0.6139123439788818,
      "learning_rate": 0.00015890124018638638,
      "loss": 16.693,
      "step": 480
    },
    {
      "epoch": 1.0040211108318673,
      "grad_norm": 0.5358321666717529,
      "learning_rate": 0.00015534276660350175,
      "loss": 16.7988,
      "step": 500
    },
    {
      "epoch": 1.0040211108318673,
      "eval_loss": 5.836112976074219,
      "eval_runtime": 338.1274,
      "eval_samples_per_second": 1.307,
      "eval_steps_per_second": 1.307,
      "step": 500
    },
    {
      "epoch": 1.0442322191505404,
      "grad_norm": 0.5700353980064392,
      "learning_rate": 0.00015168024932235617,
      "loss": 16.522,
      "step": 520
    },
    {
      "epoch": 1.0844433274692133,
      "grad_norm": 0.41215842962265015,
      "learning_rate": 0.0001479205738289176,
      "loss": 16.5678,
      "step": 540
    },
    {
      "epoch": 1.1246544357878865,
      "grad_norm": 0.5999572277069092,
      "learning_rate": 0.00014407080826535896,
      "loss": 16.7718,
      "step": 560
    },
    {
      "epoch": 1.1648655441065594,
      "grad_norm": 0.6179393529891968,
      "learning_rate": 0.00014013819014204075,
      "loss": 16.6655,
      "step": 580
    },
    {
      "epoch": 1.2050766524252325,
      "grad_norm": 0.4743661880493164,
      "learning_rate": 0.00013613011273108343,
      "loss": 16.6684,
      "step": 600
    },
    {
      "epoch": 1.2452877607439055,
      "grad_norm": 0.6220004558563232,
      "learning_rate": 0.00013205411116710972,
      "loss": 16.2796,
      "step": 620
    },
    {
      "epoch": 1.2854988690625786,
      "grad_norm": 0.8807128071784973,
      "learning_rate": 0.00012791784828128724,
      "loss": 16.6889,
      "step": 640
    },
    {
      "epoch": 1.3257099773812515,
      "grad_norm": 0.593662679195404,
      "learning_rate": 0.00012372910019530336,
      "loss": 16.4578,
      "step": 660
    },
    {
      "epoch": 1.3659210856999247,
      "grad_norm": 0.4562923312187195,
      "learning_rate": 0.00011949574170235494,
      "loss": 16.6155,
      "step": 680
    },
    {
      "epoch": 1.4061321940185976,
      "grad_norm": 0.43545615673065186,
      "learning_rate": 0.00011522573146263744,
      "loss": 16.7277,
      "step": 700
    },
    {
      "epoch": 1.4463433023372707,
      "grad_norm": 0.602835476398468,
      "learning_rate": 0.00011092709704116464,
      "loss": 16.5829,
      "step": 720
    },
    {
      "epoch": 1.4865544106559436,
      "grad_norm": 0.8626323938369751,
      "learning_rate": 0.0001066079198160484,
      "loss": 16.6615,
      "step": 740
    },
    {
      "epoch": 1.5267655189746168,
      "grad_norm": 0.5281296968460083,
      "learning_rate": 0.00010227631978561056,
      "loss": 16.4491,
      "step": 760
    },
    {
      "epoch": 1.5669766272932897,
      "grad_norm": 0.5917266011238098,
      "learning_rate": 9.794044030288922e-05,
      "loss": 16.7949,
      "step": 780
    },
    {
      "epoch": 1.6071877356119628,
      "grad_norm": 0.48835277557373047,
      "learning_rate": 9.360843276623825e-05,
      "loss": 16.8068,
      "step": 800
    },
    {
      "epoch": 1.647398843930636,
      "grad_norm": 0.4521337151527405,
      "learning_rate": 8.928844129480227e-05,
      "loss": 16.514,
      "step": 820
    },
    {
      "epoch": 1.6876099522493089,
      "grad_norm": 0.7073079347610474,
      "learning_rate": 8.498858741767579e-05,
      "loss": 16.7499,
      "step": 840
    },
    {
      "epoch": 1.7278210605679818,
      "grad_norm": 0.6192887425422668,
      "learning_rate": 8.071695480553187e-05,
      "loss": 16.6801,
      "step": 860
    },
    {
      "epoch": 1.768032168886655,
      "grad_norm": 0.7837578654289246,
      "learning_rate": 7.648157407342386e-05,
      "loss": 16.537,
      "step": 880
    },
    {
      "epoch": 1.808243277205328,
      "grad_norm": 1.021036148071289,
      "learning_rate": 7.229040768333115e-05,
      "loss": 16.5065,
      "step": 900
    },
    {
      "epoch": 1.848454385524001,
      "grad_norm": 0.3948032855987549,
      "learning_rate": 6.815133497483157e-05,
      "loss": 16.4385,
      "step": 920
    },
    {
      "epoch": 1.888665493842674,
      "grad_norm": 0.8966449499130249,
      "learning_rate": 6.407213735204343e-05,
      "loss": 16.4098,
      "step": 940
    },
    {
      "epoch": 1.928876602161347,
      "grad_norm": 0.9605405330657959,
      "learning_rate": 6.0060483654684354e-05,
      "loss": 16.2809,
      "step": 960
    },
    {
      "epoch": 1.9690877104800202,
      "grad_norm": 0.6774159669876099,
      "learning_rate": 5.6123915740750046e-05,
      "loss": 16.8374,
      "step": 980
    },
    {
      "epoch": 2.0080422216637346,
      "grad_norm": 0.5870643854141235,
      "learning_rate": 5.226983430791722e-05,
      "loss": 16.6512,
      "step": 1000
    },
    {
      "epoch": 2.0080422216637346,
      "eval_loss": 5.834661960601807,
      "eval_runtime": 337.9891,
      "eval_samples_per_second": 1.308,
      "eval_steps_per_second": 1.308,
      "step": 1000
    },
    {
      "epoch": 2.0482533299824075,
      "grad_norm": 0.41873374581336975,
      "learning_rate": 4.8505484980325685e-05,
      "loss": 16.5337,
      "step": 1020
    },
    {
      "epoch": 2.088464438301081,
      "grad_norm": 0.4333092272281647,
      "learning_rate": 4.483794468689728e-05,
      "loss": 16.3612,
      "step": 1040
    },
    {
      "epoch": 2.1286755466197538,
      "grad_norm": 0.4569820761680603,
      "learning_rate": 4.127410835679926e-05,
      "loss": 16.6839,
      "step": 1060
    },
    {
      "epoch": 2.1688866549384267,
      "grad_norm": 0.7241325974464417,
      "learning_rate": 3.7820675957064946e-05,
      "loss": 16.7895,
      "step": 1080
    },
    {
      "epoch": 2.2090977632570996,
      "grad_norm": 1.056793212890625,
      "learning_rate": 3.4484139896741e-05,
      "loss": 16.5409,
      "step": 1100
    },
    {
      "epoch": 2.249308871575773,
      "grad_norm": 0.9120715856552124,
      "learning_rate": 3.1270772821240776e-05,
      "loss": 16.3587,
      "step": 1120
    },
    {
      "epoch": 2.289519979894446,
      "grad_norm": 0.4427463114261627,
      "learning_rate": 2.8186615819850616e-05,
      "loss": 16.6215,
      "step": 1140
    },
    {
      "epoch": 2.329731088213119,
      "grad_norm": 0.33810871839523315,
      "learning_rate": 2.5237467068558728e-05,
      "loss": 16.33,
      "step": 1160
    },
    {
      "epoch": 2.3699421965317917,
      "grad_norm": 0.430509090423584,
      "learning_rate": 2.242887092955801e-05,
      "loss": 16.5354,
      "step": 1180
    },
    {
      "epoch": 2.410153304850465,
      "grad_norm": 0.35468214750289917,
      "learning_rate": 1.9766107527915523e-05,
      "loss": 16.6307,
      "step": 1200
    },
    {
      "epoch": 2.450364413169138,
      "grad_norm": 0.7971433997154236,
      "learning_rate": 1.7254182825004384e-05,
      "loss": 16.4136,
      "step": 1220
    },
    {
      "epoch": 2.490575521487811,
      "grad_norm": 0.39592504501342773,
      "learning_rate": 1.4897819207360098e-05,
      "loss": 16.7662,
      "step": 1240
    },
    {
      "epoch": 2.5307866298064843,
      "grad_norm": 0.399522066116333,
      "learning_rate": 1.2701446608653699e-05,
      "loss": 16.3648,
      "step": 1260
    },
    {
      "epoch": 2.570997738125157,
      "grad_norm": 0.32111644744873047,
      "learning_rate": 1.0669194181472863e-05,
      "loss": 16.7406,
      "step": 1280
    },
    {
      "epoch": 2.61120884644383,
      "grad_norm": 0.5093615055084229,
      "learning_rate": 8.804882534567382e-06,
      "loss": 16.4957,
      "step": 1300
    },
    {
      "epoch": 2.651419954762503,
      "grad_norm": 0.6034263372421265,
      "learning_rate": 7.1120165501533e-06,
      "loss": 16.7057,
      "step": 1320
    },
    {
      "epoch": 2.691631063081176,
      "grad_norm": 0.7395631074905396,
      "learning_rate": 5.593778794778837e-06,
      "loss": 16.7739,
      "step": 1340
    },
    {
      "epoch": 2.7318421713998493,
      "grad_norm": 0.42852360010147095,
      "learning_rate": 4.253023536139733e-06,
      "loss": 16.6536,
      "step": 1360
    },
    {
      "epoch": 2.7720532797185222,
      "grad_norm": 0.4018116295337677,
      "learning_rate": 3.092271377092215e-06,
      "loss": 16.53,
      "step": 1380
    },
    {
      "epoch": 2.812264388037195,
      "grad_norm": 0.8216263651847839,
      "learning_rate": 2.113704516951587e-06,
      "loss": 16.7583,
      "step": 1400
    },
    {
      "epoch": 2.8524754963558685,
      "grad_norm": 0.4601152837276459,
      "learning_rate": 1.3191626489853615e-06,
      "loss": 16.6065,
      "step": 1420
    },
    {
      "epoch": 2.8926866046745414,
      "grad_norm": 0.5968759059906006,
      "learning_rate": 7.101395018132007e-07,
      "loss": 16.4759,
      "step": 1440
    },
    {
      "epoch": 2.9328977129932143,
      "grad_norm": 0.35373038053512573,
      "learning_rate": 2.877800312160783e-07,
      "loss": 16.6194,
      "step": 1460
    },
    {
      "epoch": 2.9731088213118873,
      "grad_norm": 0.30877986550331116,
      "learning_rate": 5.287826763398229e-08,
      "loss": 16.6072,
      "step": 1480
    }
  ],
  "logging_steps": 20,
  "max_steps": 1494,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1018360260090921e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
